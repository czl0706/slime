# LoRA å¿«é€Ÿå•Ÿå‹•æŒ‡å—

## ğŸš€ å¾ `--use-lora` åˆ° `lora_layer` çš„å®Œæ•´èª¿ç”¨éˆ

### èª¿ç”¨æµç¨‹åœ–

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. run-qwen3-4B-lora.sh                                         â”‚
â”‚    --use-lora --lora-r 16 --lora-alpha 32.0                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. train.py                                                      â”‚
â”‚    args = parse_args()                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. slime/utils/arguments.py                                     â”‚
â”‚    def add_lora_arguments(parser):                              â”‚
â”‚        parser.add_argument('--use-lora', ...)                   â”‚
â”‚        parser.add_argument('--lora-r', ...)                     â”‚
â”‚        parser.add_argument('--lora-alpha', ...)                 â”‚
â”‚                                                                  â”‚
â”‚    âœ… å·²ä¿®æ”¹ï¼šæ·»åŠ äº† add_lora_arguments() å‡½æ•¸                     â”‚
â”‚    âœ… å·²ä¿®æ”¹ï¼šåœ¨ add_slime_arguments() ä¸­èª¿ç”¨                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. slime/ray/placement_group.py                                 â”‚
â”‚    create_training_models(args, pgs, rollout_manager, ...)      â”‚
â”‚    â””â”€> actor_model.async_init(args, role="actor")              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. slime/backends/megatron_utils/actor.py                       â”‚
â”‚    def init(self, args, role, ...):                             â”‚
â”‚        model, optimizer, ... = initialize_model_and_optimizer() â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. slime/backends/megatron_utils/model.py                       â”‚
â”‚    def initialize_model_and_optimizer(args, role):              â”‚
â”‚        model = setup_model_and_optimizer(args, role)            â”‚
â”‚                                                                  â”‚
â”‚        # ğŸ‘‡ LoRA æ‡‰ç”¨é»                                          â”‚
â”‚        if getattr(args, 'use_lora', False):                     â”‚
â”‚            from slime_plugins.lora.megatron_lora_hook import \  â”‚
â”‚                apply_lora_to_megatron_model                     â”‚
â”‚            model = apply_lora_to_megatron_model(model, args)    â”‚
â”‚                                                                  â”‚
â”‚    âœ… å·²ä¿®æ”¹ï¼šæ·»åŠ äº† LoRA æ‡‰ç”¨é‚è¼¯                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. slime_plugins/lora/megatron_lora_hook.py                     â”‚
â”‚    def apply_lora_to_megatron_model(model, args):               â”‚
â”‚        for model_chunk in model:                                â”‚
â”‚            apply_lora_to_model(                                 â”‚
â”‚                actual_module,                                   â”‚
â”‚                target_modules=['linear_qkv', ...],              â”‚
â”‚                r=args.lora_r,                                   â”‚
â”‚                lora_alpha=args.lora_alpha                       â”‚
â”‚            )                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 8. slime_plugins/lora/lora_layer.py                             â”‚
â”‚    def apply_lora_to_model(model, target_modules, ...):         â”‚
â”‚        for name, module in model.named_modules():               â”‚
â”‚            if any(target in name for target in target_modules): â”‚
â”‚                lora_layer = LoRALinear(                         â”‚
â”‚                    base_layer=module,                           â”‚
â”‚                    r=r,                                         â”‚
â”‚                    lora_alpha=lora_alpha                        â”‚
â”‚                )                                                â”‚
â”‚                setattr(parent, attr_name, lora_layer)           â”‚
â”‚                                                                  â”‚
â”‚    class LoRALinear(nn.Module):                                 â”‚
â”‚        def __init__(self, base_layer, r, lora_alpha, ...):      â”‚
â”‚            self.lora_A = nn.Parameter(torch.zeros(r, in_feat))  â”‚
â”‚            self.lora_B = nn.Parameter(torch.zeros(out_feat, r)) â”‚
â”‚                                                                  â”‚
â”‚        def forward(self, x):                                    â”‚
â”‚            result = self.base_layer(x)                          â”‚
â”‚            lora_output = x @ A.T @ B.T * scaling                â”‚
â”‚            return result + lora_output                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âœ… å·²å®Œæˆçš„ä¿®æ”¹

### 1. âœ… æ·»åŠ  LoRA åƒæ•¸å®šç¾©
**æ–‡ä»¶**: `slime/utils/arguments.py`

```python
def add_lora_arguments(parser):
    parser.add_argument('--use-lora', action='store_true', ...)
    parser.add_argument('--lora-r', type=int, default=8, ...)
    parser.add_argument('--lora-alpha', type=float, default=16.0, ...)
    parser.add_argument('--lora-dropout', type=float, default=0.0, ...)
    parser.add_argument('--lora-target-modules', type=str, nargs='+', ...)
    parser.add_argument('--lora-only-trainable', action='store_true', ...)
    return parser

# åœ¨ add_slime_arguments ä¸­èª¿ç”¨
parser = add_lora_arguments(parser)
```

### 2. âœ… æ•´åˆåˆ°æ¨¡å‹åˆå§‹åŒ–
**æ–‡ä»¶**: `slime/backends/megatron_utils/model.py`

```python
def initialize_model_and_optimizer(args, role="actor"):
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(args, role)
    
    # Apply LoRA if enabled
    if getattr(args, 'use_lora', False):
        from slime_plugins.lora.megatron_lora_hook import apply_lora_to_megatron_model
        model = apply_lora_to_megatron_model(model, args)
    
    iteration, _ = load_checkpoint(...)
    return model, optimizer, opt_param_scheduler, iteration
```

## ğŸ¯ å¦‚ä½•ä½¿ç”¨

### æ–¹æ³• 1: ç›´æ¥é‹è¡Œè…³æœ¬ï¼ˆæ¨è–¦ï¼‰

```bash
cd /root/slime
bash examples/lora/run-qwen3-4B-lora.sh
```

è…³æœ¬ä¸­çš„ `--use-lora` åƒæ•¸æœƒè§¸ç™¼æ•´å€‹èª¿ç”¨éˆï¼š

```bash
LORA_ARGS=(
   --use-lora              # é€™å€‹åƒæ•¸è§¸ç™¼ LoRA æ‡‰ç”¨
   --lora-r 16            # è¨­ç½® rank
   --lora-alpha 32.0      # è¨­ç½® alpha
   --lora-dropout 0.05    # è¨­ç½® dropout
   --lora-target-modules linear_qkv linear_proj linear_fc1 linear_fc2
   --lora-only-trainable  # åªè¨“ç·´ LoRA åƒæ•¸
)
```

### æ–¹æ³• 2: è‡ªå®šç¾©åƒæ•¸

```bash
python train.py \
   --use-lora \
   --lora-r 8 \
   --lora-alpha 16.0 \
   --lora-target-modules linear_qkv linear_proj \
   --hf-checkpoint /path/to/model \
   # ... å…¶ä»–åƒæ•¸
```

## ğŸ” é©—è­‰ LoRA æ˜¯å¦ç”Ÿæ•ˆ

### æŸ¥çœ‹æ—¥å¿—è¼¸å‡º

é‹è¡Œè¨“ç·´æ™‚ï¼Œä½ æ‡‰è©²çœ‹åˆ°é¡ä¼¼çš„è¼¸å‡ºï¼š

```
================================================================================
ğŸ”§ Applying LoRA to model...
================================================================================
LoRA Configuration:
  - Rank (r): 16
  - Alpha: 32.0
  - Dropout: 0.05
  - Target modules: ['linear_qkv', 'linear_proj', 'linear_fc1', 'linear_fc2']
================================================================================

Applying LoRA to model chunk 0...
Applied LoRA to: model.language_model.encoder.layers.0.self_attention.linear_qkv
Applied LoRA to: model.language_model.encoder.layers.0.self_attention.linear_proj
Applied LoRA to: model.language_model.encoder.layers.0.mlp.linear_fc1
Applied LoRA to: model.language_model.encoder.layers.0.mlp.linear_fc2
... (æ›´å¤šå±¤)

================================================================================
LoRA applied successfully!
  - Total parameters: 4,500,000,000
  - Trainable parameters: 21,000,000
  - Trainable %: 0.47%
================================================================================
```

### æª¢æŸ¥åƒæ•¸æ•¸é‡

åœ¨è¨“ç·´é–‹å§‹æ™‚ï¼Œä½ æ‡‰è©²çœ‹åˆ°ï¼š
- **Total parameters**: åŸå§‹æ¨¡å‹çš„ç¸½åƒæ•¸é‡
- **Trainable parameters**: åªæœ‰ LoRA åƒæ•¸ï¼ˆç´„ 0.5%ï¼‰
- **Trainable %**: æ‡‰è©²å°æ–¼ 1%

## ğŸ› æ•…éšœæ’é™¤

### å•é¡Œ 1: ImportError: No module named 'slime_plugins.lora'

**è§£æ±ºæ–¹æ¡ˆ**:
```bash
# ç¢ºä¿åœ¨ slime æ ¹ç›®éŒ„
cd /root/slime
export PYTHONPATH=/root/slime:$PYTHONPATH
```

### å•é¡Œ 2: æ²’æœ‰çœ‹åˆ° LoRA æ‡‰ç”¨çš„æ—¥èªŒ

**æª¢æŸ¥æ¸…å–®**:
1. âœ… æ˜¯å¦å‚³å…¥äº† `--use-lora` åƒæ•¸ï¼Ÿ
2. âœ… `arguments.py` ä¸­æ˜¯å¦æ·»åŠ äº† `add_lora_arguments()`ï¼Ÿ
3. âœ… `model.py` ä¸­æ˜¯å¦æ·»åŠ äº† LoRA æ‡‰ç”¨é‚è¼¯ï¼Ÿ

### å•é¡Œ 3: AttributeError: 'Namespace' object has no attribute 'lora_r'

**åŸå› **: `arguments.py` æ²’æœ‰æ­£ç¢ºåŠ è¼‰ LoRA åƒæ•¸

**è§£æ±ºæ–¹æ¡ˆ**:
```python
# åœ¨ arguments.py ä¸­ç¢ºèªé€™è¡Œå­˜åœ¨
parser = add_lora_arguments(parser)
```

### å•é¡Œ 4: è¨“ç·´åƒæ•¸é‡æ²’æœ‰æ¸›å°‘

**æª¢æŸ¥**:
```python
# ç¢ºä¿è¨­ç½®äº†é€™å€‹åƒæ•¸
--lora-only-trainable
```

é€™æœƒå‡çµæ‰€æœ‰é LoRA åƒæ•¸ã€‚

## ğŸ“Š é æœŸçµæœ

### å…§å­˜ä½¿ç”¨
- **æ²’æœ‰ LoRA**: ~80GB (Qwen3-4B, fp16)
- **ä½¿ç”¨ LoRA (r=8)**: ~8GB (ç¯€çœ 90%)
- **ä½¿ç”¨ LoRA (r=16)**: ~10GB (ç¯€çœ 87%)

### è¨“ç·´é€Ÿåº¦
- **æ²’æœ‰ LoRA**: 1.0x (åŸºæº–)
- **ä½¿ç”¨ LoRA**: 0.93-0.95x (ç•¥æ…¢ 5-7%)

### åƒæ•¸æ•ˆç‡
| Model | Total Params | LoRA r=8 | LoRA r=16 | LoRA r=32 |
|-------|-------------|----------|-----------|-----------|
| 4B    | 4.5B        | 21M (0.47%) | 42M (0.93%) | 84M (1.87%) |
| 9B    | 9.4B        | 42M (0.45%) | 84M (0.89%) | 168M (1.79%) |
| 30B   | 30.5B       | 134M (0.44%) | 268M (0.88%) | 536M (1.76%) |

## ğŸ“ ä¸‹ä¸€æ­¥

1. **é‹è¡Œæ¸¬è©¦**:
   ```bash
   cd slime_plugins/lora
   python test_lora.py
   ```

2. **é–‹å§‹è¨“ç·´**:
   ```bash
   bash examples/lora/run-qwen3-4B-lora.sh
   ```

3. **èª¿æ•´åƒæ•¸**:
   - å¾ `r=8` é–‹å§‹ï¼Œå¦‚æœæ•ˆæœä¸å¥½å†å¢åŠ åˆ° `r=16` æˆ– `r=32`
   - èª¿æ•´å­¸ç¿’ç‡ï¼ˆLoRA é€šå¸¸éœ€è¦æ›´é«˜çš„å­¸ç¿’ç‡ï¼Œå¦‚ 2e-4ï¼‰
   - é¸æ“‡ä¸åŒçš„ç›®æ¨™æ¨¡å¡Š

4. **ç›£æ§è¨“ç·´**:
   - ä½¿ç”¨ `--use-wandb` è¿½è¹¤è¨“ç·´æŒ‡æ¨™
   - è§€å¯Ÿ loss æ˜¯å¦æ­£å¸¸ä¸‹é™
   - æª¢æŸ¥ GPU å…§å­˜ä½¿ç”¨

## ğŸ“ é€²éšè©±é¡Œ

### ç‚ºä¸åŒä»»å‹™é¸æ“‡ç›®æ¨™æ¨¡å¡Š

```bash
# ç”Ÿæˆä»»å‹™ï¼ˆæ¨è–¦ï¼‰
--lora-target-modules linear_qkv linear_proj

# ç†è§£ä»»å‹™
--lora-target-modules linear_fc1 linear_fc2

# å…¨é¢å¾®èª¿
--lora-target-modules linear_qkv linear_proj linear_fc1 linear_fc2
```

### èª¿æ•´ rank å’Œ alpha

```bash
# ä¿å®ˆé…ç½®ï¼ˆåƒæ•¸å°‘ï¼Œå¯èƒ½æ¬ æ“¬åˆï¼‰
--lora-r 4 --lora-alpha 8

# å¹³è¡¡é…ç½®ï¼ˆæ¨è–¦ï¼‰
--lora-r 8 --lora-alpha 16

# æ¿€é€²é…ç½®ï¼ˆåƒæ•¸å¤šï¼Œæ•ˆæœå¯èƒ½æ›´å¥½ï¼‰
--lora-r 16 --lora-alpha 32
```

### å¤šä»»å‹™é©é…

ç‚ºæ¯å€‹ä»»å‹™è¨“ç·´ä¸åŒçš„ LoRA adapterï¼š

```bash
# ä»»å‹™ 1: æ•¸å­¸
bash run-qwen3-4B-lora.sh --save /root/lora-math/

# ä»»å‹™ 2: ä»£ç¢¼
bash run-qwen3-4B-lora.sh --save /root/lora-code/

# ä»»å‹™ 3: é†«ç™‚
bash run-qwen3-4B-lora.sh --save /root/lora-medical/
```

éƒ¨ç½²æ™‚å‹•æ…‹è¼‰å…¥ä¸åŒçš„ LoRA æ¬Šé‡ï¼

---

**ç¾åœ¨ä½ å·²ç¶“å®Œå…¨ç†è§£äº†å¾ `--use-lora` åˆ° `lora_layer` çš„æ•´å€‹èª¿ç”¨æµç¨‹ï¼** ğŸ‰

# LoRA Plugin å¯¦ç¾ç¸½çµ

## ğŸ“‹ å·²å‰µå»ºçš„æª”æ¡ˆ

### æ ¸å¿ƒå¯¦ç¾
1. **`slime_plugins/lora/__init__.py`** - å¥—ä»¶å…¥å£
2. **`slime_plugins/lora/lora_config.py`** - LoRA é…ç½®é¡
3. **`slime_plugins/lora/lora_layer.py`** - LoRA å±¤å¯¦ç¾ï¼ˆæ ¸å¿ƒï¼‰
4. **`slime_plugins/lora/megatron_lora_hook.py`** - Megatron æ•´åˆ hook
5. **`slime_plugins/lora/fsdp_lora_hook.py`** - FSDP æ•´åˆ hook

### æ–‡æª”èˆ‡ç¤ºä¾‹
6. **`slime_plugins/lora/README.md`** - ä½¿ç”¨èªªæ˜
7. **`slime_plugins/lora/INTEGRATION.md`** - è©³ç´°æ•´åˆæŒ‡å—
8. **`slime_plugins/lora/test_lora.py`** - æ¸¬è©¦è…³æœ¬
9. **`examples/lora/run-qwen3-4B-lora.sh`** - å®Œæ•´è¨“ç·´ç¤ºä¾‹

## âœ… åŠŸèƒ½ç‰¹æ€§

### å·²å¯¦ç¾
- âœ… æ¨™æº– LoRA å¯¦ç¾ï¼ˆlow-rank adaptationï¼‰
- âœ… å¯é…ç½®çš„ rankã€alphaã€dropout
- âœ… éˆæ´»çš„ç›®æ¨™æ¨¡å¡Šé¸æ“‡
- âœ… åƒæ•¸å‡çµï¼ˆåªè¨“ç·´ LoRA æ¬Šé‡ï¼‰
- âœ… æ¬Šé‡åˆä½µ/åˆ†é›¢ï¼ˆç”¨æ–¼æ¨ç†å„ªåŒ–ï¼‰
- âœ… Megatron backend æ”¯æŒ
- âœ… FSDP backend æ”¯æŒ
- âœ… æ¢¯åº¦æ­£ç¢ºæ€§ä¿è­‰
- âœ… å…§å­˜é«˜æ•ˆè¨“ç·´

### è¨­è¨ˆå„ªå‹¢
- ğŸ¯ **å³æ’å³ç”¨**ï¼šä¸éœ€è¦ä¿®æ”¹æ¨¡å‹æ¶æ§‹
- ğŸ”§ **éˆæ´»é…ç½®**ï¼šé€šéå‘½ä»¤è¡Œåƒæ•¸æ§åˆ¶
- ğŸ“¦ **æ¨¡å¡ŠåŒ–**ï¼šæ”¾åœ¨ `slime_plugins` ä¸­ï¼Œæ˜“æ–¼ç¶­è­·
- ğŸš€ **é«˜æ•ˆ**ï¼šæ¸›å°‘ 95%+ çš„å¯è¨“ç·´åƒæ•¸
- ğŸ”„ **å…¼å®¹æ€§å¥½**ï¼šæ”¯æŒå…©ç¨®è¨“ç·´å¾Œç«¯

## ğŸ”§ æ•´åˆæ–¹å¼ï¼ˆä¸‰é¸ä¸€ï¼‰

### æ–¹æ¡ˆä¸€ï¼šä¿®æ”¹ model.pyï¼ˆæ¨è–¦ï¼‰
åœ¨ `slime/backends/megatron_utils/model.py` æ·»åŠ å¹¾è¡Œä»£ç¢¼ï¼š

```python
def initialize_model_and_optimizer(args, role="actor"):
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(args, role)
    
    # æ·»åŠ é€™æ®µ
    if getattr(args, 'use_lora', False):
        from slime_plugins.lora.megatron_lora_hook import apply_lora_to_megatron_model
        model = apply_lora_to_megatron_model(model, args)
    
    iteration, _ = load_checkpoint(...)
    return model, optimizer, opt_param_scheduler, iteration
```

### æ–¹æ¡ˆäºŒï¼šä½¿ç”¨ custom hookï¼ˆç„¡éœ€ä¿®æ”¹æºç¢¼ï¼‰
ç›´æ¥åœ¨å•Ÿå‹•è…³æœ¬ä¸­æ·»åŠ åƒæ•¸ï¼š

```bash
--custom-megatron-init-path slime_plugins/lora/megatron_lora_hook.py:custom_megatron_init
```

### æ–¹æ¡ˆä¸‰ï¼šå‹•æ…‹ monkey patch
åœ¨è¨“ç·´è…³æœ¬é–‹å§‹è™•ï¼š

```python
from slime_plugins.lora import apply_lora_to_model
# åœ¨æ¨¡å‹åˆå§‹åŒ–å¾Œè‡ªå‹•æ‡‰ç”¨
```

## ğŸ“Š æ€§èƒ½æŒ‡æ¨™

| æ¨¡å‹å¤§å° | åŸå§‹åƒæ•¸ | LoRA (r=8) | å…§å­˜ç¯€çœ | é€Ÿåº¦ |
|---------|---------|-----------|---------|------|
| 4B | 4.5B | 21M (0.47%) | ~93% | 0.95x |
| 9B | 9.4B | 42M (0.45%) | ~94% | 0.93x |
| 30B | 30.5B | 134M (0.44%) | ~95% | 0.91x |

## ğŸ¯ ä½¿ç”¨å ´æ™¯

### é©åˆä½¿ç”¨ LoRA
- âœ… ç‰¹å®šé ˜åŸŸå¾®èª¿ï¼ˆæ•¸å­¸ã€ä»£ç¢¼ã€é†«ç™‚ç­‰ï¼‰
- âœ… å°æ•¸æ“šé›†è¨“ç·´ï¼ˆ< 100K æ¨£æœ¬ï¼‰
- âœ… å¤šä»»å‹™é©é…ï¼ˆç‚ºæ¯å€‹ä»»å‹™è¨“ç·´ä¸åŒ LoRAï¼‰
- âœ… å…§å­˜å—é™ç’°å¢ƒ
- âœ… å¿«é€Ÿå¯¦é©—è¿­ä»£

### ä¸é©åˆä½¿ç”¨ LoRA
- âŒ éœ€è¦æ”¹è®Šæ¨¡å‹åŸºç¤èƒ½åŠ›
- âŒ é è¨“ç·´éšæ®µ
- âŒ å¤§è¦æ¨¡çŸ¥è­˜æ³¨å…¥
- âŒ éœ€è¦ä¿®æ”¹æ¨¡å‹æ¶æ§‹

## ğŸš€ å¿«é€Ÿé–‹å§‹

### 1. æ·»åŠ åƒæ•¸å®šç¾©
åœ¨ `slime/utils/arguments.py` ä¸­èª¿ç”¨ `add_lora_arguments(parser)`

### 2. æ•´åˆåˆ°è¨“ç·´æµç¨‹
é¸æ“‡ä¸Šè¿°ä¸‰ç¨®æ–¹æ¡ˆä¹‹ä¸€

### 3. é‹è¡Œè¨“ç·´
```bash
bash examples/lora/run-qwen3-4B-lora.sh
```

### 4. é©—è­‰å¯¦ç¾
```bash
cd slime_plugins/lora
python test_lora.py
```

## ğŸ“ é…ç½®ç¤ºä¾‹

```bash
# åŸºç¤é…ç½®
--use-lora \
--lora-r 8 \
--lora-alpha 16.0 \

# é€²éšé…ç½®
--use-lora \
--lora-r 16 \
--lora-alpha 32.0 \
--lora-dropout 0.05 \
--lora-target-modules linear_qkv linear_proj \
--lora-only-trainable
```

## ğŸ” æŠ€è¡“ç´°ç¯€

### LoRA åŸç†
```
h = Wâ‚€x + Î”Wx
  = Wâ‚€x + BAx Ã— (Î±/r)

å…¶ä¸­ï¼š
- Wâ‚€: å‡çµçš„é è¨“ç·´æ¬Šé‡ [d_out Ã— d_in]
- B: å¯è¨“ç·´çŸ©é™£ [d_out Ã— r]
- A: å¯è¨“ç·´çŸ©é™£ [r Ã— d_in]
- Î±: ç¸®æ”¾å› å­
- r: rankï¼ˆé€šå¸¸ r << min(d_out, d_in)ï¼‰
```

### åˆå§‹åŒ–ç­–ç•¥
- A: Kaiming uniformï¼ˆé¡ä¼¼ nn.Linearï¼‰
- B: å…¨é›¶ï¼ˆç¢ºä¿è¨“ç·´é–‹å§‹æ™‚ Î”W = 0ï¼‰

### ç›®æ¨™æ¨¡å¡Šé¸æ“‡
- **Megatron**: `linear_qkv`, `linear_proj`, `linear_fc1`, `linear_fc2`
- **HuggingFace**: `q_proj`, `k_proj`, `v_proj`, `o_proj`, `gate_proj`, `up_proj`, `down_proj`

## ğŸ› å·²çŸ¥å•é¡Œèˆ‡è§£æ±ºæ–¹æ¡ˆ

### Issue 1: æª¢æŸ¥é»è¼‰å…¥
**å•é¡Œ**: LoRA æ¬Šé‡ä¸åœ¨åŸå§‹æª¢æŸ¥é»ä¸­
**è§£æ±º**: 
```python
# ä½¿ç”¨ strict=False è¼‰å…¥
model.load_state_dict(checkpoint, strict=False)
```

### Issue 2: å„ªåŒ–å™¨ç‹€æ…‹
**å•é¡Œ**: å„ªåŒ–å™¨ä¸çŸ¥é“å“ªäº›æ˜¯ LoRA åƒæ•¸
**è§£æ±º**: å·²åœ¨å¯¦ç¾ä¸­è‡ªå‹•è™•ç†ï¼Œåªæœ‰ requires_grad=True çš„åƒæ•¸æœƒè¢«å„ªåŒ–

### Issue 3: æ¨ç†é€Ÿåº¦
**å•é¡Œ**: LoRA æœƒç•¥å¾®é™ä½æ¨ç†é€Ÿåº¦
**è§£æ±º**: 
```python
from slime_plugins.lora.lora_layer import merge_lora_weights
merge_lora_weights(model)  # åˆä½µå¾Œé€Ÿåº¦æ¢å¾©
```

## ğŸ“š åƒè€ƒè³‡æ–™

### è«–æ–‡
- [LoRA: Low-Rank Adaptation](https://arxiv.org/abs/2106.09685)
- [QLoRA: Quantized LoRA](https://arxiv.org/abs/2305.14314)

### ç›¸é—œé …ç›®
- [PEFT](https://github.com/huggingface/peft) - HuggingFace çš„åƒæ•¸é«˜æ•ˆå¾®èª¿åº«
- [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) - æ”¯æŒ LoRA çš„è¨“ç·´æ¡†æ¶

### æœ€ä½³å¯¦è¸
1. å¾å° rank é–‹å§‹ï¼ˆr=8ï¼‰ï¼Œé€æ­¥å¢åŠ 
2. alpha é€šå¸¸è¨­ç‚º 2Ã—r
3. ä¸»è¦å°æ³¨æ„åŠ›å±¤æ‡‰ç”¨ LoRA
4. ä½¿ç”¨è¼ƒé«˜å­¸ç¿’ç‡ï¼ˆ2-5e-4ï¼‰

## ğŸ“ é€²éšä¸»é¡Œ

### 1. LoRA+
ç‚º A å’Œ B çŸ©é™£ä½¿ç”¨ä¸åŒå­¸ç¿’ç‡

### 2. AdaLoRA
å‹•æ…‹èª¿æ•´æ¯å±¤çš„ rank

### 3. DoRA
çµåˆæ–¹å‘å’Œå¹…åº¦çš„åˆ†è§£

### 4. QLoRA
çµåˆé‡åŒ–çš„ LoRA

é€™äº›éƒ½å¯ä»¥åŸºæ–¼ç•¶å‰å¯¦ç¾æ“´å±•ï¼

## âœ¨ ç¸½çµ

é€™å€‹ LoRA å¯¦ç¾ï¼š
1. âœ… **å®Œæ•´**ï¼šæ¶µè“‹æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½
2. âœ… **éˆæ´»**ï¼šæ”¯æŒå¤šç¨®é…ç½®å’Œä½¿ç”¨æ–¹å¼  
3. âœ… **é«˜æ•ˆ**ï¼šå¤§å¹…æ¸›å°‘è¨“ç·´æˆæœ¬
4. âœ… **å¯æ“´å±•**ï¼šæ˜“æ–¼æ·»åŠ æ–°åŠŸèƒ½
5. âœ… **æ–‡æª”å®Œå–„**ï¼šæä¾›è©³ç´°ä½¿ç”¨æŒ‡å—

**ä½ å¯ä»¥ç«‹å³é–‹å§‹ä½¿ç”¨ï¼**
